<meta charset="UTF-8">

<h1>Big data</h1>
<h3>Origem: Wikipédia, a enciclopédia livre.</h3>
<br><br>
	
<p>O Big Data abrange conjuntos de dados de grande volume, variedade e velocidade.</p>

<p>Big Data (megadados ou grandes dados em português[1]) é a área do conhecimento que estuda como tratar, analisar e obter informações a partir de conjuntos de dados grandes demais para serem analisados por sistemas tradicionais. Ao longo das últimas décadas, a quantidade de dados gerados tem crescido de forma exponencial.O surgimento da Internet aumentou de forma abrupta a quantidade de dados produzidos, e a popularização da Internet das coisas fez sairmos da era do terabyte para o petabyte[2][3]. Em 2015, entramos na era do zetabytes[4], e atualmente geramos mais de 2,5 quintilhões de bytes diariamente[5]. O termo Big Data surgiu em 1997[6] e seu uso foi utilizado para nomear essa quantidade cada vez mais crescente e não estruturada de dados sendo gerados a cada segundo. Atualmente o big data é essencial nas relações econômicas e sociais e representou uma evolução nos sistemas de negócio e na ciência[3]. As ferramentas de big data são de grande importância na definição de estratégias de marketing, aumentar a produtividade, reduzir custos e tomar decisões mais inteligentes[7]. A essência do conceito está em gerar valor para negócios[8]. No que tange a ciência, o surgimento do big data representou a criação de um novo paradigma (4° paradigma) sendo concebido um novo método de avançar as fronteiras do conhecimento, por meio de novas tecnologias para coletar, manipular, analisar e exibir dados, construindo valor agregado com as análises geradas[9].</p>
<nav> <ul>
    
    <h4>Índice</h4>

    <li>1 Definição</li>
    <li>2 Histórico</li>
    <li>3 Tipos de Dados</li>
    <li>4 Mercado de trabalho</li>
    <li>5 Aplicações na atualidade</li>
    <li>6 Críticas</li>
    <li>7 Ver Também</li>
    <li>8 Referências</li>
    <li>9 Bibliografia</li>
    <li>10 Ligações externas</li>

</ul>
</nav>

<h2>Definição</h2>

<p>Big data é um termo recente e por isso não existente na maior parte de dicionários de estatística. São dados multivariados e de elevada dimensão, geralmente criados em tempo real e apresentam um crescimento exponencial (na escala temporal), nomeados de megadados[10].</p>
<p>Quanto mais dados são gerados, maior é o esforço para extrair informações[8], e os centros de dados tiveram que aprender a lidar com o crescimento exponencial de dados gerados e tiveram que desenvolver ferramentas que fossem para além de bancos de dados relacionais e sistemas paralelos de bancos de dados[3]. Sendo assim, a velocidade para obter a informação faz parte do sucesso que o big data pode proporcionar em sua empresa[8]. O conceito de big data foi definido inicialmente por 3'V[8] mas a literatura mostrou que seu conceito pode ser expandido para 5'V[11], representados pelos seguintes conceitos[8]:</p>

    <p>Volume: relacionado a grande quantidade de dados gerados;</p>
    <p>as fontes de dados são muito variadas, o que aumenta a complexidade das análises;</p>
    <p>Velocidade: Devido ao grande volume e variedade de dados, todo o processamento deve ser ágil para gerar as informações necessárias;</p>
    <p>Veracidade: A veracidade está ligada diretamente ao quanto uma informação é verdadeira.</p>
    <p>Valor: Este conceito está relacionado com o valor obtido desses dados, ou seja, com a “informação útil”.</p>

<h2>Histórico</h2>

<h4>Construção do Conceito</h4>

<p>O termo big data tem um conceito relativo, já que seu tamanho depende de quem está usando os dados[12]. Neste contexto, o primeiro relato sobre uso de estatísticas para obter informações de grandes quantidades de dados data de 1663. Nesse ano, John Graunt utilizou uma grande quantidade de informações, de diferentes fontes, para estudar a epidemia da peste bulbônica na Europa. Para Graunt, sua quantidade de dados poderia ser considerado big data[13].</p>

<p>O uso dos primeiros equipamentos para processar dados datam de 1890, durante a realização do Censo dos Estados Unidos, conduzido pelo U.S. Census Bureau[14]. Na ocasião, a Máquina de Tabulação diminuiu o tempo de processamento dos dados para apenas 6 semanas[15]. Entretanto, somente em no século XX que começaram a surgir os primeiros sistemas para armazenamento de informações. Em 1927, o engenheiro Fritz Pfleumer criou um método para guardar informações em fitas magnéticas[12].</p>

<p>Durante a Segunda Guerra Mundial, foi criada a primeira máquina digital de processamento de dados. Foi em 1943, quando os Britânicos desenvolveram um sistema para decifrar códigos nazistas durante a Segunda Guerra Mundial. O nome da máquina era Colossus, que podia interceptar mensagens a uma taxa de 5000 caracteres por segundo[16]. A primeiro órgão público criado especificamente para o processamento de dados, a Agência Nacional de Segurança (NSA) dos EUA, foi fundado em 1952, com o objetivo de processar dados automaticamente para obter informações relativas a inteligência durante a Guerra Fria[17].</p>

<p>Um dos primeiros Centro de Dados foi criado em 1965, também pelo governo americano, com o objetivo de controlar o pagamento de impostos e as impressões digitais dos americanos[16]. Este Centro de Dados possuía o mesmo padrão dos bancos de dados criados até a década de 1970. Eram bancos de dados centralizados, onde uma mesma máquina era responsável pelo uso, armazenamento e análise dos dados[3]. Com o aumento da quantidade de dados, começaram a surgir novas arquiteturas de dados que permitissem processar e analisar esses dados. Nas década de 80 começaram a surgir os Sistemas de Bancos de Dados Paralelos[18]. Nesse caso, ao invés de um banco de dados centralizado, cada processador se comunica com os outros apenas enviando mensagens através de uma rede interconectada. Os primeiros bancos de dados paralelos possibilitaram a criação do primeiro banco de dados com capacidade em terabytes, pela KMART, em 1986[3].</p>

<p>Em 1989, o cientista britânico Tim Berners-Lee criou o World Wide Web, para facilitar a troca de informações entre as pessoas. O que Tim Berners-Lee não sabia era que sua invenção iria revolucionar a forma como os dados eram gerados e a quantidade de dados criados[19]. A criação da Web 2.0 ajudou no aumento dos dados[16]. O termo big data foi usado pela primeira vez em 1997[6], entretanto o nome começou a ser usado oficialmente em 2005, quando Roger Mougalas, da O’Reilly Media publicou um artigo mencionando o tema[20].</p>

<h4>Evolução Tecnológica de Armazenamento e Processamento</h4>

<p>Os dados que agregam o conjunto do big data são provenientes de várias fontes. Desta maneira, normalmente não apresenta uma estrutura bem definida, ou seja, não pode ser armazenada nos sistemas padrões de banco de dados, como o Sistema Gerenciador de Banco de Dados Relacional (SGBDR), onde os dados são representados por meio de tabelas, com diversas linhas e colunas[21]. Os cientistas de dados começaram a verificar que bancos de dados relacionais não conseguiriam suportar essa grande quantidade de dados não estruturados. Desta maneira, novas tecnologias e processos tiveram que ser desenvolvidos para permitir que esses dados não estruturados fossem analisados, já que os mesmos podem representar até 80% do total de dados[22]. Foi quando a Google criou o MapReduce, em 2004[23], que é um modelo de programação que permite processar grandes quantidades de dados em paralelo, dividindo o trabalho em um conjunto de tarefas independentes, geralmente executado em um cluster de computadores[24].</p>

<p>Posteriormente, foi desenvolvido o Hadoop, que é uma implementação em código aberto do MapReduce[25]. O Hadoop foi criado pelo Yahoo em 2005 e pode ser considerado uma das maiores invenções de data management desde o modelo relacional[26]. Entretanto, o Hadoop não é considerado uma base dados como o SGBDR. Ele é um sistema de distribuição de arquivos utilizado para processar e armazenas grande quantidade de dados (big data) por meio de clusters[21], onde os mesmos são processados paralelamente e podendo ser executados em servidores sem muito esforço[25]. Atualmente, esse tipo de processamento é o mais utilizado por empresas que trabalham com big data e diversas empresas vêm contribuindo com código para seu desenvolvimento, como a Yahoo, Facebook, Cloudera, IBM e outras[26].</p>

<p>Segundo a IBM em 2008 foram produzidos cerca de 2,5 quintilhões de bytes todos os dias e surpreendentemente 90% dos dados no mundo foram criados nos últimos dois anos, decorrente a adesão das grandes empresas à internet, como exemplo as redes sociais, dados dos GPS, dispositivos embutidos e móveis[27]. Atualmente, a Internet das Coisas mudou a forma como os dados são gerados, aumentando de forma abrupta a quantidade de dados gerados[3]. Todos esses objetos físicos da Internet das Coisas são capazes de coletar e transmitir dados, gerando dados não estruturados que não podem ser armazenados e processados por banco de dados comuns.</p>

<h2>Tipos de Dados</h2>

<p>Existem tipos básicos de dados que são estudados pelos especialistas em big data, os conceitos mais utilizados geralmente envolvem:</p>

<p>Social Data: Dados coletados de redes sociais ou ambientes de interação entre usuários, geralmente demográficos e comportamentais, ou seja, ditam um padrão de um determinado grupo com as mesmas característica. O Social Data é muito utilizado na análise de campanhas de marketing, de maneira a oferecer um serviço ou produto mais personalizado de acordo com diferentes segmentos.</p>
<p>Enterprise Data: Na tradução literal Dados Empresariais, coletados pelo RH de empresas, setores de vendas, finanças, logística e produção, esses dados são atributos sobre funcionários e setores diferentes dentro de um ambiente empresarial, podem ser utilizados para otimizar processos e identificar falhas ou fraudes dentro de uma determinada seção, esse tipo de dado é um marco de investimento estratégico de grandes empresas, que visam minimizar gastos e otimizar lucros.</p>
<p>Personal Data: Dados pessoais, facilmente relacionados ao conceito da Internet das coisas, são dados obtidos através de aparelhos de uso pessoal ou coletivo, tais como smartphones, geladeiras, televisões, carros, etc. Esse tipo de dado mostra as preferências pessoais de um determinado indivíduo através do estudo de padrões, por meio do uso do Personal Data é possível desenvolver metodologias personalizadas de interação com o cliente, de maneira a tornar a relação com o produto menos mecanizada e robotizada.</p>

<h2>Mercado de trabalho</h2>

<p>As oportunidades de trabalho na área de estatística estão aumentando graças à proliferação de programas para análise de dados e seu uso, especialmente, na tomada de decisão com objetivos estratégicos como: políticas de governo, seleção de investimentos, gestão de empresas e negócios, etc. O big data permite trabalhar com grandes volumes de dados, por vezes, não aceitos pelos grandes programas estatísticos. No Brasil existe a profissão de Estatístico, regulamentada pelo Decreto Federal nº 62497 de 1968[28]. Este profissional é treinado para trabalhar com estruturas de dados, em seu manuseio para extração de informação estratégica, nos métodos estatísticos de análise e em programação para sua análise estatística, de modo a se obter conclusões com margens de erro controladas para a tomada de decisões com base nos dados disponíveis. A IBM criou a Big Data University, que fornece certo conhecimento do big data. Existem na Internet, sites que oferecem plataformas de ensino à distância, comumente conhecidas como MOOCs, com cursos nas áreas de big data e de ciência de dados (Data Science, no original em inglês), nos quais pode-se estudar o seu conteúdo de forma gratuita ou pagar pelo certificado do curso. Os mais conhecidos são os sites do Coursera, Udacity e o EDX.org, este último, fruto de parceria entre as universidades americanas de Harvard e do MIT e empresas do Vale do Silício. No Brasil, o mercado para a área é promissor, sendo que muitas renomadas Universidades passaram a oferecer cursos de pós-graduação e MBAs ligados à área de big data, variando em sua maioria no tamanho da carga horária destinada à parte de negócios, componente importante na formação deste profissional, que precisará ter além das habilidades técnicas, a capacidade de apresentar as conclusões de suas análises e insights para um público leigo de forma simples, de forma a gerar valor para o negócio da empresa.</p>
<h3>Aplicações na atualidade</h3>

<p>Um estudo do Instituto IDC mostrou que diversos setores da sociedade estão investindo em big data[29], indicando que foram investidos mais de US$ 16,6 bilhões em 2014 para atividades do setor. Este mesmo estudo afirma que a expectativa é que este valor atinja, em 2018, o valor de US$ 41,5 bilhões. As instituições estão investindo em big data por observarem da interferência dos custos, das consequências que pode haver para o futuro do negócio. O objetivo por trás do big data é melhorar a prestação de informações aos gestores, fazendo com que haja um suporte na tomada de decisões – com dados reais e precisos[30]. A seguir serão apresentadas algumas aplicações de big data, em diferentes setores:</p>

    <blockquote cite="https://pt.wikipedia.org/wiki/Big_data"> O filme “Moneyball” (O homem que mudou o jogo) com o ator Brad Pitt, no qual o gerente de um time de beisebol usa o big data para reunir um time de primeira linha sem gastar muito;
    A empresa UPS, após análise das rotas de seus motoristas, proibiu os mesmos de virar à esquerda[31]. De acordo com a empresa, isto permitiu economizar por ano cerca de 38 milhões de litros de combustível, deixando de emitir 20 mil toneladas de dióxido de carbono. Além disso, entregam 350 mil pacotes a mais;
    No terremoto do Haiti, pesquisadores americanos fizeram uso da geolocalização de 2 milhões de chips SIM, para auxiliar nas missões humanitárias[32];
    Para melhorar os laboratórios de física nuclear, a empresa CERN (Organização Europeia para a Pesquisa Nuclear), criou o maior acelerador de partículas do mundo, chamado Large Hadron Collider. Com ele, é gerada uma quantidade enorme de dados. Para a utilização dessa máquina é necessário muita memória - cerca de 30 petabytes de dados - e, para analisar esses dados são necessários 65 mil processadores, e usa também o recurso de vários computadores pelo mundo inteiro.
    A utilização dos dados de censos e outros recolhidos pelos governos, facilita na análise dos Datas Censes, melhorando a nossa saúde e também ciência social.[33]
    Em busca dos melhores lugares para instalar turbinas eólicas, uma empresa dinamarquesa analisou petabytes de dados climáticos do nível das marés, mapas de desmatamentos, entre outros. No fim o que costumava demorar semanas durou apenas algumas horas[34];
    Big data foi de grande importância para o descobrimento do pré-sal, devido a sua velocidade, que agilizava os processamentos de dados sísmicos captados pelas sondas que procuram petróleo no fundo do mar. Como são milhões as variáveis, o trabalho exige intermináveis simulações de imagens, e só o big data é capaz de dar conta do trabalho em um tempo melhor[35];
    Alguns times de diversos esportes utilizam o big data na performance dos atletas, com câmeras e outros aparelhos. Desta maneira, conseguem observar o desempenho dos atletas e, ao analisar os dados, tomar decisões mais precisas, melhorando o desempenho e corrigindo os erros, criando também estatísticas para os próximos jogos[36];
    Empresas de tecnologia como a Netflix e a Spotify utilizam de big data para definir as preferências dos seus usuários, e fornecer para eles conteúdos mais individualizados[37];
    As ferramentas de propaganda do Facebook e do Instagram são baseadas em big data, pois correlacionam dados dos usuários das redes sociais com suas preferências de consumos e serviços[38].
    Um interessante estudo de caso sobre sucessos e erros do uso do big data é o Google Flu Trends (GTF), que foi lançado pela Google em 2008[39]. Este serviço foi divulgado pela primeira vez por meio de um artigo na revista Nature[40], e prometia detectar com algumas semanas de antecedência a ocorrência de epidemias de gripe. Anteriormente ao GFT, os sistemas tradicionais dos EUA faziam estimativas de casos e epidemias de gripe a cada duas semanas, usando dados dos Centros de Controle e Prevenção de Doenças dos EUA (U.S. Centers for Disease Control and Prevention - CDC). Essas estimativas eram baseadas em dados virológicos e clínicos, relacionados a visitas de pacientes aos hospitais e consultórios. Com o aumento do acesso à internet, verificou-se que mais de 90 milhões de americanos procuravam todos os anos por informações sobre uma doença específica ou problema médico. Nesse contexto, os pesquisadores da Google verificaram que era possível correlacionar essas buscas com casos efetivos de gripe[40]. Para validar a metodologia, foram processadas centenas de bilhões de pesquisas no Google pelo período de 5 anos (2003 a 2007) para os EUA, correlacionando palavras de busca específicas com casos efetivos da doença. Os dados foram validados por meio dos relatórios da CDC para o período, com um correlação média de 90%[40]. O modelo foi testado em tempo real nos anos de 2007 e 2008 e os resultados foram divulgados com o CDC para avaliar a resposta e a acurácia, mostraram a possibilidade de prever casos de gripe em uma a duas semanas antes do CDC[40]. A partir desses resultados, o serviço começou a ser utilizado operacionalmente para outros países, realizando estimativas de epidemias de gripe para mais de 25 nações[39]. Entretanto, atualmente o serviço não está mais ativo, mas estimativas históricas ainda estão disponíveis para download[39]. Isso aconteceu pelos erros subsequentes nas previsões realizadas pelo serviço nos anos posteriores. Isso aconteceu em 2013, quando o sistema não previu uma epidemia de gripe[41], ou como a epidemia da gripe H1N1, em 2009[42]. Em um artigo publicado na revista Science[43], pesquisadores indicaram os seguintes fatores como os causadores dos problemas e das falhas com o serviço GFT:
            A "arrogância do big data" (Big data hubris). Este termo é utilizado para descrever a máxima normalmente utilizada pelos cientistas de dados, que consideram o big data como um substituto aos dados e análises tradicionais, ao invés de considerá-lo uma análise complementar e conjunta;
            A dinâmica do algoritmo de busca da Google, que mudou ao longo dos anos e pode ter afetado o resultado das tendências
            A falta de transparência e impossibilidade de replicabilidade dos resultados. Apesar da Google ter divulgado a metodologia, os dados utilizados não são públicos, o que não permite replicar os resultados obtidos e avaliar melhores formas de ajustar os algoritmos utilizados no programa.</blockquote>

<h2>Críticas</h2>

<p>A massificação de dados, no entanto, ainda enfrenta obstáculos. O maior deles seria a privacidade, ou seja, a ameaça à privacidade representada pelo aumento de armazenamento e integração de informações pessoalmente identificáveis. Se a recomendação de links patrocinados pelo Google já parece invasiva à maioria das pessoas, o mundo e a legislação atual não estão preparadas para as possibilidades que o big data oferece de agregar, analisar e tirar conclusões a partir de dados até então esparsos. Painéis de especialistas lançaram várias recomendações de políticas para adequar a prática às expectativas de privacidade. [44][45][46]</p>

<p>Outro problema é a escassez de profissionais, que terão de se adaptar a tal tecnologia, com a previsão que em 2018 só os Estados Unidos podem enfrentar a falta de 140mil a 190mil com profundas capacidades analíticas.</p>

<p>O big data já foi relacionado[47][48] como ferramenta essencial em manipulação de eleições e disseminação de fake news, isso se dá pela capacidade inerente da tecnologia de reunir e segmentar um determinado público alvo, fazendo com que campanhas de marketing sejam muito mais efetivas e impactantes, isso faz do big data uma metodologia questionável do ponto de vista ético, uma vez que pode ser usado para manipular massas e obter resultados parciais de acordo com a motivação dos especialistas.</p>
<ul><h3>Ver Também</h3>

    <li><a href="https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados">Ciência de dados</a></li>
    <li><a href="https://pt.wikipedia.org/wiki/Visualiza%C3%A7%C3%A3o_de_dados">Visualização de dados</a></li>
    <li><a href="https://pt.wikipedia.org/wiki/Data%C3%ADsmo">Dataísmo</a></li>
    <li><a href="https://pt.wikipedia.org/wiki/Internet_das_coisas">Internet das coisas</a></li>
</ul>